{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Build your own convolutional neural network using pytorch"
      ],
      "metadata": {
        "id": "FYdu3SsKbbBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "import os\n",
        "import csv"
      ],
      "metadata": {
        "id": "yp1wcgCfIbRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSfENMgmH8Q0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf3fd61-1c93-42ab-dca7-bc755f246a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomCNN(\n",
            "  (conv_layers): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU()\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): ReLU()\n",
            "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (18): ReLU()\n",
            "    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (20): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU()\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc_layers): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.5, inplace=False)\n",
            "    (9): Linear(in_features=256, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=10):\n",
        "        super(CustomCNN, self).__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(1024 * 1 * 1, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(256, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "model = CustomCNN(in_channels=3, num_classes=10)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Train your model using dog heart dataset"
      ],
      "metadata": {
        "id": "DtH0zo_RcoAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg7gsfzPKPmE",
        "outputId": "dce15361-dc23-46a2-8815-6a8d4cd77c98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dog_heart_path = '/content/drive/MyDrive/Dog_heart'\n",
        "train_path = os.path.join(dog_heart_path, 'Train')\n",
        "valid_path = os.path.join(dog_heart_path, 'Valid')\n",
        "test_path = '/content/drive/MyDrive/Test'"
      ],
      "metadata": {
        "id": "GjVgumcbKV9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((75, 75)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "train_dataset = ImageFolder(root=train_path, transform=data_transform)\n",
        "print(f'Train dataset loaded successfully with {len(train_dataset)} images and {len(train_dataset.classes)} classes')\n",
        "\n",
        "valid_dataset = ImageFolder(root=valid_path, transform=data_transform)\n",
        "print(f'Valid dataset loaded successfully with {len(valid_dataset)} images and {len(valid_dataset.classes)} classes')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-RgJ8nWKYzT",
        "outputId": "31536834-a409-4025-affb-bef3674dc239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset loaded successfully with 1400 images and 3 classes\n",
            "Valid dataset loaded successfully with 200 images and 3 classes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if fname.endswith(('jpg', 'jpeg', 'png'))]\n",
        "        print(f\"Found {len(self.image_files)} images in {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, os.path.basename(img_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            raise\n",
        "\n",
        "test_dataset = TestDataset(root_dir=test_path, transform=data_transform)\n",
        "print(f'Test dataset loaded successfully with {len(test_dataset)} images')\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNBuXPpOKnXN",
        "outputId": "bbf0dbd8-aa2a-456e-bb48-61caa55f1429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400 images in /content/drive/MyDrive/Test\n",
            "Test dataset loaded successfully with 400 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CustomCNN(in_channels=3, num_classes=len(train_dataset.classes)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 50\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}')\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss = val_loss / len(valid_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/custom_cnn_dog_heart.pth')\n",
        "\n",
        "output_csv_path = '/content/drive/MyDrive/custom_cnn_predictions.csv'\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad(), open(output_csv_path, mode='w', newline='') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    for images, file_names in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        for file_name, pred in zip(file_names, predicted):\n",
        "            csv_writer.writerow([file_name, pred.item()])\n",
        "\n",
        "print(f'Predictions saved to {output_csv_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "End4kf_eO43m",
        "outputId": "c7064314-7ddb-4a4f-da8f-777894e5b456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Train Loss: 1.0411\n",
            "Epoch [1/50], Val Loss: 1.0742\n",
            "Epoch [2/50], Train Loss: 0.9061\n",
            "Epoch [2/50], Val Loss: 2.0863\n",
            "Epoch [3/50], Train Loss: 0.8412\n",
            "Epoch [3/50], Val Loss: 2.3401\n",
            "Epoch [4/50], Train Loss: 0.7615\n",
            "Epoch [4/50], Val Loss: 0.7524\n",
            "Epoch [5/50], Train Loss: 0.7390\n",
            "Epoch [5/50], Val Loss: 0.7479\n",
            "Epoch [6/50], Train Loss: 0.6996\n",
            "Epoch [6/50], Val Loss: 0.7582\n",
            "Epoch [7/50], Train Loss: 0.6844\n",
            "Epoch [7/50], Val Loss: 0.8675\n",
            "Epoch [8/50], Train Loss: 0.6822\n",
            "Epoch [8/50], Val Loss: 0.6851\n",
            "Epoch [9/50], Train Loss: 0.6181\n",
            "Epoch [9/50], Val Loss: 0.6704\n",
            "Epoch [10/50], Train Loss: 0.6161\n",
            "Epoch [10/50], Val Loss: 0.7614\n",
            "Epoch [11/50], Train Loss: 0.5564\n",
            "Epoch [11/50], Val Loss: 0.8355\n",
            "Epoch [12/50], Train Loss: 0.5761\n",
            "Epoch [12/50], Val Loss: 0.7631\n",
            "Epoch [13/50], Train Loss: 0.5254\n",
            "Epoch [13/50], Val Loss: 0.7137\n",
            "Epoch [14/50], Train Loss: 0.5154\n",
            "Epoch [14/50], Val Loss: 0.6576\n",
            "Epoch [15/50], Train Loss: 0.4449\n",
            "Epoch [15/50], Val Loss: 0.7569\n",
            "Epoch [16/50], Train Loss: 0.4059\n",
            "Epoch [16/50], Val Loss: 0.9851\n",
            "Epoch [17/50], Train Loss: 0.4665\n",
            "Epoch [17/50], Val Loss: 1.3483\n",
            "Epoch [18/50], Train Loss: 0.3946\n",
            "Epoch [18/50], Val Loss: 0.6489\n",
            "Epoch [19/50], Train Loss: 0.3001\n",
            "Epoch [19/50], Val Loss: 0.7782\n",
            "Epoch [20/50], Train Loss: 0.2798\n",
            "Epoch [20/50], Val Loss: 0.8782\n",
            "Epoch [21/50], Train Loss: 0.2875\n",
            "Epoch [21/50], Val Loss: 0.7824\n",
            "Epoch [22/50], Train Loss: 0.2419\n",
            "Epoch [22/50], Val Loss: 1.1199\n",
            "Epoch [23/50], Train Loss: 0.3041\n",
            "Epoch [23/50], Val Loss: 0.6704\n",
            "Epoch [24/50], Train Loss: 0.1795\n",
            "Epoch [24/50], Val Loss: 1.0703\n",
            "Epoch [25/50], Train Loss: 0.1773\n",
            "Epoch [25/50], Val Loss: 1.2201\n",
            "Epoch [26/50], Train Loss: 0.1554\n",
            "Epoch [26/50], Val Loss: 1.1285\n",
            "Epoch [27/50], Train Loss: 0.1435\n",
            "Epoch [27/50], Val Loss: 1.3437\n",
            "Epoch [28/50], Train Loss: 0.1969\n",
            "Epoch [28/50], Val Loss: 1.0577\n",
            "Epoch [29/50], Train Loss: 0.1122\n",
            "Epoch [29/50], Val Loss: 1.1958\n",
            "Epoch [30/50], Train Loss: 0.0930\n",
            "Epoch [30/50], Val Loss: 1.3486\n",
            "Epoch [31/50], Train Loss: 0.0811\n",
            "Epoch [31/50], Val Loss: 1.1414\n",
            "Epoch [32/50], Train Loss: 0.0598\n",
            "Epoch [32/50], Val Loss: 1.0816\n",
            "Epoch [33/50], Train Loss: 0.0763\n",
            "Epoch [33/50], Val Loss: 1.4139\n",
            "Epoch [34/50], Train Loss: 0.0574\n",
            "Epoch [34/50], Val Loss: 1.7631\n",
            "Epoch [35/50], Train Loss: 0.0558\n",
            "Epoch [35/50], Val Loss: 1.4553\n",
            "Epoch [36/50], Train Loss: 0.0554\n",
            "Epoch [36/50], Val Loss: 1.4367\n",
            "Epoch [37/50], Train Loss: 0.0317\n",
            "Epoch [37/50], Val Loss: 1.3901\n",
            "Epoch [38/50], Train Loss: 0.0499\n",
            "Epoch [38/50], Val Loss: 1.3277\n",
            "Epoch [39/50], Train Loss: 0.0839\n",
            "Epoch [39/50], Val Loss: 1.2023\n",
            "Epoch [40/50], Train Loss: 0.0948\n",
            "Epoch [40/50], Val Loss: 1.0806\n",
            "Epoch [41/50], Train Loss: 0.0845\n",
            "Epoch [41/50], Val Loss: 1.6069\n",
            "Epoch [42/50], Train Loss: 0.0375\n",
            "Epoch [42/50], Val Loss: 1.6992\n",
            "Epoch [43/50], Train Loss: 0.1138\n",
            "Epoch [43/50], Val Loss: 0.9412\n",
            "Epoch [44/50], Train Loss: 0.1614\n",
            "Epoch [44/50], Val Loss: 2.0673\n",
            "Epoch [45/50], Train Loss: 0.0590\n",
            "Epoch [45/50], Val Loss: 1.5814\n",
            "Epoch [46/50], Train Loss: 0.0694\n",
            "Epoch [46/50], Val Loss: 1.5466\n",
            "Epoch [47/50], Train Loss: 0.0387\n",
            "Epoch [47/50], Val Loss: 2.0154\n",
            "Epoch [48/50], Train Loss: 0.0710\n",
            "Epoch [48/50], Val Loss: 1.3227\n",
            "Epoch [49/50], Train Loss: 0.0462\n",
            "Epoch [49/50], Val Loss: 1.8154\n",
            "Epoch [50/50], Train Loss: 0.0262\n",
            "Epoch [50/50], Val Loss: 3.7079\n",
            "Predictions saved to /content/drive/MyDrive/custom_cnn_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Evaluate your model using the developed software"
      ],
      "metadata": {
        "id": "GTpPOwd-dLA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2025-03-23 145547.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "kK9Q7I0WKIpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Compare results with [RVT paper](https://www.nature.com/articles/s41598-023-50063-x). Requirement: performance is better than VGG16: 75%"
      ],
      "metadata": {
        "id": "SXNffNxxKi9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom CNN model achieved an accuracy of 73.25%, which is competitive with VGG16 (74.8%) and demonstrates the potential of a simpler, custom designed architecture for cardiomegaly detection in dogs. While the RVT model outperforms both with 87.3% accuracy, it leverages advanced transformer based architectures and pre-trained weights, which are computationally intensive.\n",
        "\n",
        "Analyzing model's performance\n",
        "\n",
        "> Simplicity and Efficiency: Our lightweight custom CNN achieves near VGG16 performance with only six convolutional layers, making it efficient and deployable in resource-constrained settings.\n",
        "\n",
        "> Class Imbalance: The dataset is highly imbalanced (e.g., large heart images represent only 11.5%), yet our model performs well, showcasing its robustness.\n",
        "\n",
        "> No pre-trained weights: Unlike VGG16 and RVT, our model was trained from scratch, making its performance even more impressive.\n",
        "\n",
        "> Potential for improvement: With data augmentation, larger datasets, and advanced techniques, our model can surpass VGG16 and approach RVT’s performance.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "While RVT sets a high benchmark, our custom CNN demonstrates that a simpler model can achieve competitive results close to VGG16. This highlights its potential for real-world veterinary applications, with room for further improvement to enhance accuracy and scalability."
      ],
      "metadata": {
        "id": "PfaYI2WpKsCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Write a four-page paper report using the shared LaTex template. Upload your paper to ResearchGate or Arxiv, and put your paper link and GitHub weight link here."
      ],
      "metadata": {
        "id": "nxSGunTm16Fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.researchgate.net/publication/390175147_CARDIOMEGALY_DETECTION_IN_DOGS_USING_CNN\n",
        "\n",
        "https://github.com/satvik-srk/Cardiomegaly_detection_using_cnn.git\n",
        "\n",
        "https://drive.google.com/file/d/1U6PpZG1rDUhnENj0XIlw8FxpY8Ek2zBz/view?usp=sharing"
      ],
      "metadata": {
        "id": "q-Og0CNf1-OA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # 6. Grading rubric\n",
        "\n",
        "(1). Code ------- 20 points (you also need to upload your final model as a pt file)\n",
        "\n",
        "(2). Grammer ---- 20 points\n",
        "\n",
        "(3). Introduction & related work --- 10 points\n",
        "\n",
        "\n",
        "(4). Method  ---- 20 points\n",
        "\n",
        "(5). Results ---- 20 points\n",
        "\n",
        "     > = 75 % -->10 points\n",
        "     < 55 % -->0 points\n",
        "     >= 55 % & < 75% --> 0.5 point/percent\n",
        "     \n",
        "\n",
        "(6). Discussion - 10 points"
      ],
      "metadata": {
        "id": "AMRZPOlx3ag9"
      }
    }
  ]
}